{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoiZ6HJdFJYNSIAV7Fexbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivas2234/Applied-Data-Science-with-Python-Specialization/blob/main/LeNets_AlexNet_VGG16_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGClXSZQWEr4"
      },
      "outputs": [],
      "source": [
        "\n",
        "Computer vision: LeNet-5, AlexNet, VGG-19, GoogLeNet\n",
        "Import various modules that we need for this notebook.\n",
        "\n",
        "In [3]:\n",
        "%pylab inline\n",
        "\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist, cifar10\n",
        "from keras.models import Sequential, Graph\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from keras.optimizers import SGD, RMSprop\n",
        "from keras.utils import np_utils\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from PIL import Image\n",
        "Using Theano backend.\n",
        "/Users/taylor/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
        "  warnings.warn(\"downsample module has been moved to the pool module.\")\n",
        "Populating the interactive namespace from numpy and matplotlib\n",
        "Load the MNIST dataset, flatten the images, convert the class labels, and scale the data.\n",
        "\n",
        "In [4]:\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32') / 255\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32') / 255\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "I. LeNet-5 for MNIST10\n",
        "Here is my attempt to replicate the LeNet-5 model as closely as possibly the original paper: LeCun, Yann, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86, no. 11 (1998): 2278-2324.\n",
        "\n",
        "As few modern neural network libraries allow for partially connected convolution layers, I've substituted this with a dropout layer. I've also replaced momentum with the Hessian approximation, and rescaled the learning rate schedule, though the proportional decay remains the same.\n",
        "\n",
        "In [4]:\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(6, 5, 5, border_mode='valid', input_shape = (1, 28, 28)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(120, 1, 1, border_mode='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(84))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "In [12]:\n",
        "l_rate = 1\n",
        "sgd = SGD(lr=l_rate, mu=0.8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=2,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "\n",
        "sgd = SGD(lr=0.8 * l_rate, mu=0.8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=3,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "\n",
        "sgd = SGD(lr=0.4 * l_rate, mu=0.8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=3,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "\n",
        "sgd = SGD(lr=0.2 * l_rate, mu=0.8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=4,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "\n",
        "sgd = SGD(lr=0.08 * l_rate, mu=0.8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=8,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/2\n",
        "60000/60000 [==============================] - 72s - loss: 0.1487 - acc: 0.9540 - val_loss: 0.0655 - val_acc: 0.9784\n",
        "\n",
        "Epoch 8/8\n",
        "60000/60000 [==============================] - 70s - loss: 0.0630 - acc: 0.9799 - val_loss: 0.0308 - val_acc: 0.9900\n",
        "Out[12]:\n",
        "<keras.callbacks.History at 0x125caffd0>\n",
        "In [13]:\n",
        "print(\"Test classification rate %0.05f\" % model.evaluate(X_test, Y_test, show_accuracy=True)[1])\n",
        "10000/10000 [==============================] - 2s     \n",
        "Test classification rate 0.99000\n",
        "And once again, let's look at the misclassified examples.\n",
        "\n",
        "In [15]:\n",
        "y_hat = model.predict_classes(X_test)\n",
        "test_wrong = [im for im in zip(X_test,y_hat,y_test) if im[1] != im[2]]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for ind, val in enumerate(test_wrong[:100]):\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "    plt.subplot(10, 10, ind + 1)\n",
        "    im = 1 - val[0].reshape((28,28))\n",
        "    plt.axis(\"off\")\n",
        "    plt.text(0, 0, val[2], fontsize=14, color='blue')\n",
        "    plt.text(8, 0, val[1], fontsize=14, color='red')\n",
        "    plt.imshow(im, cmap='gray')\n",
        "10000/10000 [==============================] - 2s     \n",
        "\n",
        "II. LeNet-5 with \"Distortions\" (i.e., Data augmentation)\n",
        "The LeNet paper also introduced the idea of adding tweaks to the input data set in order to artificially increase the trainin set size. They suggested slightly distorting the image by shifting or stretching the pixels. The idea is that these distortions should not change the output image classification. Keras has a pre-built library for doing this; let us try to use it here to improve the classification rate. Note that we do not want to flip the image, as this would change the meaning of some digits (6 & 9, for example). Minor rotations are okay, however.\n",
        "\n",
        "In [40]:\n",
        "# this will do preprocessing and realtime data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(X_train)\n",
        "We'll use the same adaptation of LeNet-5 architecture.\n",
        "\n",
        "In [41]:\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(6, 5, 5, border_mode='valid', input_shape = (1, 28, 28)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Convolution2D(120, 1, 1, border_mode='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(84))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "Now we'll fit the model. Notice that the format for this is slightly different as the data is coming from datagen.flow rather than a single numpy array. We set the number of sample per epoch to be the same as before (60k). I am also using the non-augmented version with RMS prop for the first 2 epochs, as the details are not specified in the paper and this seems to greatly improve the convergence.\n",
        "\n",
        "In [42]:\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop())\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=25,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/25\n",
        "60000/60000 [==============================] - 71s - loss: 0.6566 - acc: 0.7880 - val_loss: 0.1605 - val_acc: 0.9489\n",
        "\n",
        "Epoch 25/25\n",
        "60000/60000 [==============================] - 64s - loss: 0.0833 - acc: 0.9738 - val_loss: 0.0394 - val_acc: 0.9876\n",
        "Out[42]:\n",
        "<keras.callbacks.History at 0x12c8d5438>\n",
        "How does the performance stack up? Not quite as good as the non-distorted version, though notice how the classifier does not overfit the same was as it would without the data augmentation. I have a hunch that there is something non-optimal about the RMSprop implementation when using data augmentation.\n",
        "\n",
        "At any rate, the true advantage of data augmentation comes when we have large models (regularization) or more complex learning tasks (generalization).\n",
        "\n",
        "In [43]:\n",
        "print(\"Test classification rate %0.05f\" % model.evaluate(X_test, Y_test, show_accuracy=True)[1])\n",
        "10000/10000 [==============================] - 2s     \n",
        "Test classification rate 0.98760\n",
        "III. OverFeat adaptation of AlexNet (2012)\n",
        "An adaptation of the 'fast' model from AlexNet applied to MNIST-10.\n",
        "\n",
        "In [44]:\n",
        "model = Sequential()\n",
        "\n",
        "# Layer 1\n",
        "model.add(Convolution2D(96, 11, 11, input_shape = (1,28,28), border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Convolution2D(256, 5, 5, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Layer 4\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(1024, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Layer 5\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(1024, 3, 3, border_mode='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 6\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3072, init='glorot_normal'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Layer 7\n",
        "model.add(Dense(4096, init='glorot_normal'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Layer 8\n",
        "model.add(Dense(10, init='glorot_normal'))\n",
        "model.add(Activation('softmax'))\n",
        "As you can imagine, training this model (even on MNIST-10) is quite time consuming. I'll run just one Epoch with 10 samples to show how it works.\n",
        "\n",
        "In [45]:\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop())\n",
        "model.fit(X_train[:10], Y_train[:10], batch_size=1, nb_epoch=1,\n",
        "          verbose=1, show_accuracy=True)\n",
        "Epoch 1/1\n",
        "10/10 [==============================] - 95s - loss: nan - acc: 0.1000    \n",
        "Out[45]:\n",
        "<keras.callbacks.History at 0x119c08f28>\n",
        "The true power of this model really comes out when it is used on a larger corpus of images, such as ILSVRC and MS COCO, with images having a larger spatial size.\n",
        "\n",
        "IV. VGG-19 Model\n",
        "Now, let's load the VGG-19 model using pre-trained weights. First, we'll create a keras model as normal:\n",
        "\n",
        "In [13]:\n",
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1000, activation='softmax'))\n",
        "We then load the weights of the model from a file (you can download this from the course website; it is not small, coming in at about half a gigabyte). We then have to compile the model, even though we have no intention of actually training it. This is because the compilation in part sets the forward propigation code, which we will need to do predictions.\n",
        "\n",
        "In [14]:\n",
        "model.load_weights(\"../../../class_data/keras/vgg19_weights.h5\")\n",
        "\n",
        "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
        "We will also load some metadata, that gives class labels to the output:\n",
        "\n",
        "In [15]:\n",
        "synsets = []\n",
        "with open(\"../../../class_data/keras/synset_words.txt\", \"r\") as f:\n",
        "    synsets += f.readlines()\n",
        "synsets = [x.replace(\"\\n\",\"\") for x in synsets]\n",
        "Now lets read in an image of a lion:\n",
        "\n",
        "In [27]:\n",
        "im = Image.open('img/lion.jpg').resize((224, 224), Image.ANTIALIAS)\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(im)\n",
        "im = np.array(im).astype(np.float32)\n",
        "\n",
        "# scale the image, according to the format used in training\n",
        "im[:,:,0] -= 103.939\n",
        "im[:,:,1] -= 116.779\n",
        "im[:,:,2] -= 123.68\n",
        "im = im.transpose((2,0,1))\n",
        "im = np.expand_dims(im, axis=0)\n",
        "\n",
        "And now predict the class label from the VGG-19 model:\n",
        "\n",
        "In [28]:\n",
        "out = model.predict(im)\n",
        "for index in np.argsort(out)[0][::-1][:10]:\n",
        "    print(\"%01.4f - %s\" % (out[0][index], synsets[index].replace(\"\\n\",\"\")))\n",
        "0.3274 - n02129165 lion, king of beasts, Panthera leo\n",
        "0.2489 - n02125311 cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
        "0.2208 - n02128757 snow leopard, ounce, Panthera uncia\n",
        "0.0753 - n02128385 leopard, Panthera pardus\n",
        "0.0631 - n02128925 jaguar, panther, Panthera onca, Felis onca\n",
        "0.0360 - n02117135 hyena, hyaena\n",
        "0.0091 - n02127052 lynx, catamount\n",
        "0.0063 - n01882714 koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\n",
        "0.0024 - n02129604 tiger, Panthera tigris\n",
        "0.0020 - n01883070 wombat\n",
        "A relatively impressive result for an out of sample image!\n",
        "\n",
        "V. GoogLeNet - Inception Module\n",
        "An implementation of the Inception module, the basic building block of GoogLeNet (2014). As with OverFeat, I don't have enough compute power here to actually traing the model, but this does serve as a nice example of how to use the graph interface in keras.\n",
        "\n",
        "In [46]:\n",
        "model = Graph()\n",
        "model.add_input(name='n00', input_shape=(1,28,28))\n",
        "\n",
        "# layer 1\n",
        "model.add_node(Convolution2D(64,1,1, activation='relu'), name='n11', input='n00')\n",
        "model.add_node(Flatten(), name='n11_f', input='n11')\n",
        "\n",
        "model.add_node(Convolution2D(96,1,1, activation='relu'), name='n12', input='n00')\n",
        "\n",
        "model.add_node(Convolution2D(16,1,1, activation='relu'), name='n13', input='n00')\n",
        "\n",
        "model.add_node(MaxPooling2D((3,3),strides=(2,2)), name='n14', input='n00')\n",
        "\n",
        "# layer 2\n",
        "model.add_node(Convolution2D(128,3,3, activation='relu'), name='n22', input='n12')\n",
        "model.add_node(Flatten(), name='n22_f', input='n22')\n",
        "\n",
        "model.add_node(Convolution2D(32,5,5, activation='relu'), name='n23', input='n13')\n",
        "model.add_node(Flatten(), name='n23_f', input='n23')\n",
        "\n",
        "model.add_node(Convolution2D(32,1,1, activation='relu'), name='n24', input='n14')\n",
        "model.add_node(Flatten(), name='n24_f', input='n24')\n",
        "\n",
        "# output layer\n",
        "model.add_node(Dense(1024, activation='relu'), name='layer4',\n",
        "               inputs=['n11_f', 'n22_f', 'n23_f', 'n24_f'], merge_mode='concat')\n",
        "model.add_node(Dense(10, activation='softmax'), name='layer5', input='layer4')\n",
        "model.add_output(name='output1',input='layer5')\n",
        "In [48]:\n",
        "model.compile(loss={'output1':'categorical_crossentropy'}, optimizer=RMSprop())\n",
        "model.fit({'n00':X_train[:100], 'output1':Y_train[:100]}, nb_epoch=1, verbose=1)\n",
        "Epoch 1/1\n",
        "100/100 [==============================] - 24s - loss: 7.0162\n",
        "Out[48]:\n",
        "<keras.callbacks.History at 0x156ea1b38>\n",
        "VI. Batch Normalization\n",
        "Use the Batch Normalization of: Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015). We'll re-train LeNet-5, but use relu units.\n",
        "\n",
        "In [38]:\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(6, 5, 5, border_mode='valid', input_shape = (1, 28, 28)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Convolution2D(120, 1, 1, border_mode='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(84))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "In [39]:\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop())\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=8,\n",
        "          verbose=1, show_accuracy=True, validation_data=(X_test, Y_test))\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/8\n",
        "60000/60000 [==============================] - 73s - loss: 0.2294 - acc: 0.9293 - val_loss: 0.0937 - val_acc: 0.9697\n",
        "\n",
        "Epoch 8/8\n",
        "60000/60000 [==============================] - 89s - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0520 - val_acc: 0.9857\n",
        "Out[39]:\n",
        "<keras.callbacks.History at 0x12a0ee8d0>\n",
        "VII. Residual block - as in ResNet (2015)\n",
        "An example of the residual block used in the pre-print: \"Deep Residual Learning for Image Recognition.\" (2015).\n",
        "\n",
        "In [36]:\n",
        "model = Graph()\n",
        "model.add_input(name='input0', input_shape=(1,28,28))\n",
        "model.add_node(Flatten(), name='input1', input='input0')\n",
        "model.add_node(Dense(50),   name='input2', input='input1')\n",
        "\n",
        "model.add_node(Dense(50, activation='relu'), name='middle1', input='input2')\n",
        "model.add_node(Dense(50, activation='relu'), name='middle2', input='middle1')\n",
        "\n",
        "model.add_node(Dense(512, activation='relu'), name='top1',\n",
        "               inputs=['input2', 'middle2'], merge_mode='sum')\n",
        "model.add_node(Dense(10, activation='softmax'), name='top2', input='top1')\n",
        "model.add_output(name='top3',input='top2')\n",
        "In [37]:\n",
        "model.compile(loss={'top3':'categorical_crossentropy'}, optimizer=RMSprop())\n",
        "model.fit({'input0':X_train, 'top3':Y_train}, nb_epoch=25, verbose=1,\n",
        "          validation_data={'input0':X_test, 'top3':Y_test})\n",
        "Train on 60000 samples, validate on 10000 samples\n",
        "Epoch 1/25\n",
        "60000/60000 [==============================] - 3s - loss: 0.3205 - val_loss: 0.1624\n",
        "\n",
        "Epoch 25/25\n",
        "60000/60000 [==============================] - 2s - loss: 0.0081 - val_loss: 0.1210\n",
        "Out[37]:\n",
        "<keras.callbacks.History at 0x115ced2e8>\n",
        "VIII. Pure convolution\n",
        "For reference, here is the architecture of a Pure Convolution network: Springenberg, J. T., Dosovitskiy, A., Brox, T., & Riedmiller, M. (2014). Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806.\n",
        "\n",
        "In [6]:\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(96, 5, 5, border_mode='valid', input_shape = (1, 28, 28)))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Convolution2D(192, 5, 5, border_mode='valid'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Convolution2D(192, 3, 3, border_mode='valid'))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Convolution2D(192, 1, 1, border_mode='valid'))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Convolution2D(10, 1, 1, border_mode='valid'))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "          \n",
        "rms = RMSprop()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=rms)\n",
        "In [ ]:\n",
        " "
      ]
    }
  ]
}